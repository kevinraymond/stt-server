# GPU Dockerfile for older NVIDIA drivers (legacy support)
# Build: docker build -f Dockerfile.gpu-legacy -t obsidian-stt:gpu-legacy .
# Run: docker run --gpus all -p 8765:8765 obsidian-stt:gpu-legacy
#
# Requires:
#   - NVIDIA driver 525+ (check with: nvidia-smi)
#   - NVIDIA Container Toolkit: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
#
# Note: Uses CUDA 12.1 + cuDNN 8 for broader driver compatibility.
# For newer drivers (545+), use Dockerfile.gpu instead for better performance.

FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04

# Install Python and system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3.11 \
    python3-pip \
    python3.11-venv \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/* \
    && ln -s /usr/bin/python3.11 /usr/bin/python

# Set working directory
WORKDIR /app

# Upgrade pip
RUN python -m pip install --upgrade pip

# Install PyTorch with CUDA support
RUN pip install --no-cache-dir \
    torch --index-url https://download.pytorch.org/whl/cu121

# Install other dependencies
# Pin ctranslate2==4.4.0 for cuDNN 8 compatibility (cuDNN 9 requires ctranslate2>=4.5.0)
RUN pip install --no-cache-dir \
    websockets>=12.0 \
    numpy>=1.24.0 \
    ctranslate2==4.4.0 \
    faster-whisper>=1.0.0

# Copy application code
COPY src/ ./src/

# Pre-download the "distil-large-v3" model (recommended for GPU)
# Download using CPU during build (model files are the same)
# At runtime, the model will load with CUDA
RUN python -c "from faster_whisper import WhisperModel; WhisperModel('distil-large-v3', device='cpu', compute_type='int8')"

# Expose WebSocket port
EXPOSE 8765

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Default command - auto-detect will choose GPU settings
CMD ["python", "-m", "src.cli", "--auto", "--host", "0.0.0.0"]
